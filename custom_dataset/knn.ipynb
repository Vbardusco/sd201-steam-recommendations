{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ca36ea",
   "metadata": {},
   "source": [
    "# SD201 project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fe45d",
   "metadata": {},
   "source": [
    "# Recommendations based on hours played : k-nn on players\n",
    "The aim of the algorithm is to predict a list of games someone may like knowing how much he played to other games.\n",
    "It means that the feature we want to predict is a list of games, and the features used to do so are, for each game in the database, the amount of hours spent playing this game.\n",
    "\n",
    "To do so, we will use our own dataset created from scratch. If you are interested by our progression and the justification of the choices we made to create our model, please consult the knn_progress.ipynb notebook in the kaggle_dataset section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's gather the dataset we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>game</th>\n",
       "      <th>hours_played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76561198006667424</td>\n",
       "      <td>240</td>\n",
       "      <td>2677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76561198006667424</td>\n",
       "      <td>4000</td>\n",
       "      <td>57279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76561198006667424</td>\n",
       "      <td>4760</td>\n",
       "      <td>19541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76561198006667424</td>\n",
       "      <td>4770</td>\n",
       "      <td>1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76561198006667424</td>\n",
       "      <td>10500</td>\n",
       "      <td>35248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id   game  hours_played\n",
       "0  76561198006667424    240          2677\n",
       "1  76561198006667424   4000         57279\n",
       "2  76561198006667424   4760         19541\n",
       "3  76561198006667424   4770          1720\n",
       "4  76561198006667424  10500         35248"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the data\n",
    "#read data\n",
    "data = pd.read_json('SteamGameData.json')\n",
    "#clean data\n",
    "data.columns = ['id','game','hours_played']\n",
    "played_games = data.loc[data['hours_played'] > 0]\n",
    "played_games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fc130",
   "metadata": {},
   "source": [
    "As we only got the appid and not the game name, we will use another dataset which will gives us the name of the games. However, as our dataset is huge, we will only use it when making predictions and not on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Counter-Strike: Source', 240)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steam_games_info = pd.read_csv('steam.csv')\n",
    "\n",
    "def game_name(appid):\n",
    "    try:\n",
    "        return steam_games_info.loc[steam_games_info['appid']==appid]['name'].values[0]\n",
    "    except:\n",
    "        return 'Unknown game'\n",
    "\n",
    "def appid(game_name):\n",
    "    try:\n",
    "        return steam_games_info.loc[steam_games_info['name']==game_name]['appid'].values[0]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "game_name(240), appid('Counter-Strike: Source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reorder the dataset by creating the dictionnaries of games played for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8061, 2842)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get a dict of games and hours played for each id\n",
    "played_dict = played_games.groupby('id').apply(lambda g : dict(zip(g['game'], g['hours_played'])))\n",
    "\n",
    "played_dict_3 = played_dict.loc[played_dict.map(len)>=3]\n",
    "played_dict_100 = played_dict.loc[played_dict.map(len)>=100]\n",
    "len(played_dict_3),len(played_dict_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also will encode the hours played and compute the games list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vectors of hours played \n",
    "hours_encoded = played_dict.apply(pd.Series)\n",
    "#Replace NaN values by 0 : a game not in the dict has never been played\n",
    "hours_encoded = hours_encoded.fillna(0)\n",
    "#We drop the ids because they are not useful anymore\n",
    "hours_encoded = hours_encoded.reset_index(drop=True)\n",
    "#Sort by name of games\n",
    "hours_encoded = hours_encoded.reindex(sorted(hours_encoded.columns),axis=1)\n",
    "\n",
    "games_list = list(hours_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to get the model of predictions we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteamPredictionModel():\n",
    "    \n",
    "    def __init__(self, k_neighbors = 5):\n",
    "        self.neigh = NearestNeighbors(n_neighbors=k_neighbors, metric='euclidean')\n",
    "        self.games_list = []\n",
    "        self.likeness = None\n",
    "        self.mean_dict = {}\n",
    "        self.std_dict = {}\n",
    "        self.average_played = {}\n",
    "\n",
    "    def dict_to_likeness(self, dicti):\n",
    "        d = dicti.copy()\n",
    "        for game in d.keys():\n",
    "            if d[game] <= self.average_played[game]:\n",
    "                d[game]=0\n",
    "        return d\n",
    "    \n",
    "    #We fit the model on a dataset containing ids and dictionnaries of games associated with time played\n",
    "    def fit(self, data):\n",
    "\n",
    "        #Firstly we encode the hours played\n",
    "        hours_encoded = data.apply(pd.Series)\n",
    "        #Replace NaN values by 0 : a game not in the dict has never been played\n",
    "        hours_encoded = hours_encoded.fillna(0)\n",
    "        hours_encoded = hours_encoded.reindex(sorted(hours_encoded.columns),axis=1)\n",
    "        \n",
    "\n",
    "        #For each player, we compute the list of game he likes with the time he has played aboved average time played\n",
    "        non_zero_dict = hours_encoded.replace(0, np.NaN)\n",
    "        self.average_played = non_zero_dict.mean(axis=0)\n",
    "        likeness_games = data.map(self.dict_to_likeness)\n",
    "        #And encode them\n",
    "        likeness_games_encoded = likeness_games.apply(pd.Series)\n",
    "        #Replace NaN values by 0 : a game not in the dict has never been played\n",
    "        likeness_games_encoded = likeness_games_encoded.fillna(0)\n",
    "        likeness_games_encoded = likeness_games_encoded.reindex(sorted(likeness_games_encoded.columns),axis=1)\n",
    "\n",
    "        #standardization\n",
    "        #We standardize each column separately\n",
    "\n",
    "        def standardize(c):\n",
    "            m = c.mean()\n",
    "            if c.std() > 0:\n",
    "                std = c.std()\n",
    "            else:\n",
    "                std = 1e-8\n",
    "            return (c-m)/std\n",
    "\n",
    "        hours_encoded = hours_encoded.apply(lambda column : standardize(column),axis=0)\n",
    "\n",
    "        #we store the mean and std to standardize vectors on which we want to make predictions\n",
    "        self.mean_dict = hours_encoded.mean(axis=0)\n",
    "        self.std_dict = hours_encoded.std(axis=0)\n",
    "        self.std_dict.replace(to_replace=0,value=1e-8,inplace=True)\n",
    "\n",
    "        #we also standardize the likeness\n",
    "        likeness_games_encoded = likeness_games_encoded.apply(lambda column : standardize(column),axis=0)\n",
    "        self.likeness = likeness_games_encoded\n",
    "\n",
    "\n",
    "\n",
    "        self.neigh.fit(hours_encoded.values)\n",
    "        \n",
    "        #Get the list of games\n",
    "        games_list = list(hours_encoded.columns)\n",
    "        games_list.sort()\n",
    "        self.games_list = games_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    #We predict a certain number of games (maximum) using a dictionnary of games associated with time played\n",
    "    def predict(self, X_init, recommendations_number_max, predict_names=False):\n",
    "        #One-hot-encode X\n",
    "        X = pd.Series(X_init,index=self.games_list).fillna(0)\n",
    "        \n",
    "        #Create a vector with all games and a null score\n",
    "        score = pd.Series({self.games_list[0]:0.0},index=self.games_list).fillna(0)\n",
    "\n",
    "        #Get the list of games played by X\n",
    "        already_owned = [self.games_list[index] for index in np.asarray(X).nonzero()[0]]\n",
    "\n",
    "        \n",
    "        stand_vector = (X-self.mean_dict)/self.std_dict\n",
    "        #Get the neighbors of X\n",
    "        kneighbors = self.neigh.kneighbors([stand_vector])\n",
    "        kneighbors_distances = kneighbors[0][0]\n",
    "        kneighbors_indices = kneighbors[1][0]\n",
    "\n",
    "        for i in range(len(kneighbors_indices)):\n",
    "            neighbor = kneighbors_indices[i]\n",
    "            #get the list of liked games\n",
    "            liked = [self.games_list[index] for index in np.asarray(self.likeness.iloc[neighbor]).nonzero()[0]]\n",
    "            #Add to each game score (1/d)*l with d the distance between X and the neighbor\n",
    "            #and l the amount of time played above the average\n",
    "            for liked_game in liked:\n",
    "                if liked_game not in already_owned:\n",
    "                    score[liked_game] = score[liked_game] + 1/kneighbors_distances[i] + self.likeness.iloc[neighbor][liked_game]\n",
    "\n",
    "\n",
    "        score = score.sort_values(ascending=False)\n",
    "        return score.iloc[:recommendations_number_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating, we will create stereotyped players (focused on a single genre of games for instance) in order to manually test our results.  \n",
    "Note : the game names are not the same as in the kaggle dataset, and the time played is not in hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fit the model\n",
    "SPM = SteamPredictionModel(100)\n",
    "SPM.fit(played_dict_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpg_player = {appid('The Witcher® 3: Wild Hunt'):700,appid('The Elder Scrolls V: Skyrim'):800, appid('Far Cry® 4'):500, appid('Fallout 3'):500}\n",
    "encoded_rpg_player = pd.Series(rpg_player,index=games_list).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = SPM.predict(encoded_rpg_player,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skylar & Plux: Adventure On Clover Island\n",
      "Unknown game\n",
      "The Elder Scrolls V: Skyrim Special Edition\n",
      "RUNNING WITH RIFLES\n",
      "Metro: Last Light Redux\n",
      "Garbage Day\n",
      "Fallout 4\n",
      "Kingdom Come: Deliverance\n",
      "Far Cry 3\n",
      "ARMA: Cold War Assault\n"
     ]
    }
   ],
   "source": [
    "for game in predictions.to_dict().keys():\n",
    "    print(game_name(game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we obtain are coherent : mostly adventure games and rpgs.  \n",
    "However, we notice that the calculations are way longer (about 1min there) than in the kaggle dataset, which is logical because this one is much more dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_player = {appid('Call of Duty® 4: Modern Warfare®'):700,appid('Counter-Strike: Global Offensive'):800}\n",
    "encoded_fps_player = pd.Series(fps_player,index=games_list).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = SPM.predict(encoded_fps_player,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collider\n",
      "Unknown game\n",
      "Unknown game\n",
      "Unknown game\n",
      "Easy™ eSports\n",
      "Counter-Strike\n",
      "J.A.W.S\n",
      "Counter-Strike: Source\n",
      "Day of Defeat: Source\n",
      "Defiance\n"
     ]
    }
   ],
   "source": [
    "for game in predictions.to_dict().keys():\n",
    "    print(game_name(game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the results seem coherent with the type of player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knn evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset contains player that played a lot of games compared to the Kaggle dataset (150 games played on average in this dataset compared to 10 in the Kaggle dataset), we will reduce the evaluation to players that played at least 100 games which still represents 2842 players, and use 20 neighbors (because we have about the same number of players as in the kaggle dataset's case), otherwise the calculations are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(data, validation_size = 0.1, test_size=0.1):\n",
    "    \n",
    "    #shuffle\n",
    "    shuffled = data.sample(frac=1, random_state=42) #set seed for reproducability\n",
    "    \n",
    "    #split train and test\n",
    "    separator1 = len(shuffled) - int(len(shuffled)*(test_size+validation_size))\n",
    "    separator2 = len(shuffled) - int(len(shuffled)*(test_size))\n",
    "    X_train = shuffled.iloc[:separator1]\n",
    "    X_validation = shuffled.iloc[separator1:separator2]\n",
    "    X_test = shuffled.iloc[separator2:]\n",
    "    \n",
    "    return (X_train,X_validation,X_test)\n",
    "\n",
    "X_train, X_validation, X_test = train_validation_test_split(played_dict_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fit the model\n",
    "SPM = SteamPredictionModel(20)\n",
    "SPM.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_dict = hours_encoded.replace(0, np.NaN)\n",
    "average_played = non_zero_dict.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [51:21<00:00, 10.85s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Evaluate the model\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "\n",
    "for player in tqdm(X_test):\n",
    "    \n",
    "    player_games = len(player)\n",
    "    \n",
    "    games_to_predict = int(len(player)*5/100)\n",
    "    if games_to_predict == 0:\n",
    "        games_to_predict = 1 #we remove at least one game\n",
    "    \n",
    "    games_for_prediction = {}\n",
    "    games_removed = []\n",
    "    n = 0\n",
    "\n",
    "    for (name,time) in list(player.items()):\n",
    "        if n < games_to_predict and time >average_played[name]: #the game is liked\n",
    "            games_removed.append(name)\n",
    "            n += 1\n",
    "        else:\n",
    "            games_for_prediction[name]=time\n",
    "    \n",
    "    prediction = []\n",
    "    if len(games_removed) > 0:\n",
    "        prediction = list(SPM.predict(games_for_prediction,games_to_predict).index)\n",
    "    \n",
    "    for predicted_game in prediction:\n",
    "        if predicted_game in games_removed:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "precision = true_positives/(true_positives+false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024196597353497166"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456.0332703213611"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision/(1/len(list(hours_encoded.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the precision is lower than with the Kaggle's dataset, the algorithm seems to perform better as it performs 450x better than random compared to 200x for the Kaggle's dataset (indeed the precision is lower but there are much more games in this dataset !).  \n",
    "However, we can think that this result may be a bit biased due to the method we used to create the dataset : as we took friends of friends of friends..., it is possible that we gathered the same types of players, and thus the recommendations on others may be easier.  \n",
    "We're still satisfied with the results, but the calculation times are too high for this dataset to be used in a real model. A perspective to improve could be to try to clean the dataset but it would necessite a very meticulous process to determine which games are negligeable, how to reduce the calculation times, etc..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82beec7210971e3769f3d591826a624b1fec7874f4c33a161b0cc69971345e33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
